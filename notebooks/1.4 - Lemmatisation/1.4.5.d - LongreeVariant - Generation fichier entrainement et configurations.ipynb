{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectif\n",
    "\n",
    "L'objectif de ce notebook est de compiler la liste des oeuvres existantes, avec leur nombre de tokens, quelque soit leur utilisation dans l'entraînement général.\n",
    "\n",
    "Étapes prévues :\n",
    "\n",
    "| Numero | Type | Titre |\n",
    "| ------ | ---- | ----- |\n",
    "| 1      | Auto | Lister les oeuvres avec leurs tokens |\n",
    "| 2      | Manu | Dispatcher dans des corpus dans une feuille csv |\n",
    "| 3      | Config | Générer les corpus et les configurations |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Étape 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep only those headers in step 3\n",
    "kept_header = [\"token\", \"lemma\", \"pos\", \"Gend\"]\n",
    "\n",
    "import glob\n",
    "from collections import Counter\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "\n",
    "# Get the table\n",
    "def get_title(filename):\n",
    "    for group in re.findall(r\"([A-Z][a-z]+)([A-Z][a-zA-Z]+)?0?(\\d+)?\\.tsv\", filename):\n",
    "        return \", \".join([g for g in group if g])\n",
    "\n",
    "\n",
    "nb_tokens = Counter()\n",
    "\n",
    "for file in sorted(list(glob.glob(\"../../../LASLA/mood-tense-voice/**/*.tsv\"))):\n",
    "    # Number of tokens = number of lines that are not empty - 1 for the header\n",
    "    filename = os.path.basename(file)\n",
    "    title = get_title(filename)\n",
    "    with open(file) as f:\n",
    "        count = -1\n",
    "        for line in f:\n",
    "            count += int(bool(line.strip()))\n",
    "        nb_tokens[(title, filename)] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CSV\n",
    "import csv\n",
    "\n",
    "with open(\"1.4.5.d-lemmatisation-impact-liste-corpus-auto.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"title\", \"file\", \"tokens\"])\n",
    "    for ((title, file), cnt) in nb_tokens.items():\n",
    "        writer.writerow([title, file, cnt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 2\n",
    "\n",
    "Travail manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 3\n",
    "\n",
    "Génération des corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "corpora = defaultdict(list) # filename : [corpusname]\n",
    "\n",
    "with open(\"1.4.5.d-lemmatisation-impact-selection-corpus-manuelle.csv\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    need_build = reader.fieldnames[3:]\n",
    "    for row in reader:\n",
    "        for corpus in need_build:\n",
    "            y = row[corpus] == \"y\"\n",
    "            if y:\n",
    "                corpora[row[\"file\"]].append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_tags = [\"token\", \"lemma\", \"pos\", \"Gend\"]\n",
    "tests = []\n",
    "output_corpus = defaultdict(list)\n",
    "for file in sorted(list(glob.glob(\"../../../LASLA/mood-tense-voice/**/*.tsv\"))):\n",
    "    filename = os.path.basename(file)\n",
    "    if filename in corpora:\n",
    "        with open(file) as f:\n",
    "            headers = None\n",
    "            sentences = [[]]\n",
    "            for line_no, line in enumerate(f):\n",
    "                if line_no == 0:\n",
    "                    headers = line.strip().split(\"\\t\")\n",
    "                    continue\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Sentence end\n",
    "                    sentences.append([])\n",
    "                    continue\n",
    "                sentences[-1].append({\n",
    "                    key:val\n",
    "                    for key, val in zip(headers, line.split(\"\\t\"))\n",
    "                    if key in used_tags\n",
    "                })\n",
    "\n",
    "            for corpus in corpora[filename]:\n",
    "                if corpus == \"test\":\n",
    "                    corpus = filename.replace(\".tsv\", \"\")\n",
    "                    tests.append(corpus)\n",
    "                output_corpus[corpus].extend([s for s in sentences if s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prose', 'corpus-divers', 'Caesar_BellumGallicum_CaesBG3', 'Catullus_Catullus_Catul', 'cicero-discours', 'Cicero_DeAmicitia_CicAmici', 'Cicero_InCatilinam_CicCat1', 'ciceron-petit', 'Curtius_CurtiusHistoriaeAlexandriMagni_QCurt03', 'vers', 'horace+lucrece', 'Horatius_Epodi_HorEpodi', 'ovide', 'Ovidius_InIbin_OviIbin', 'Petronius_PetroniusSatiricon_PetronSa', 'theatre-plaute-seneque', 'PseudoCaesar1_BellumAfricanum_BAfr', 'Sallustius_Catilina_SalCatil', 'seneque-philo-autres', 'seneque-lucilium', 'Seneca_DeBrevitateVitae_SenBrevi', 'Seneca_Medea_SenMedea', 'tacite', 'Tacitus_TacGermania_TacGerma', 'virgile'])\n"
     ]
    }
   ],
   "source": [
    "print(output_corpus.keys())\n",
    "import os\n",
    "import random\n",
    "os.makedirs(\"./1.4.5.d-lemmatisation-impact/tests\", exist_ok=True)\n",
    "\n",
    "with open(\"./1.4.5.d-lemmatisation-impact-information-corpora.csv\", \"w\") as stats:\n",
    "    stats.write(\"corpus,train,dev,train_toks,dev_toks\\n\")\n",
    "    for corpus in output_corpus:\n",
    "        random.shuffle(output_corpus[corpus])\n",
    "        train = int(len(output_corpus[corpus]) * 0.95)\n",
    "        dev = len(output_corpus[corpus])-train\n",
    "        train_toks = len([tok for s in output_corpus[corpus][:train] for tok in s])\n",
    "        dev_toks = len([tok for s in output_corpus[corpus][train:] for tok in s])\n",
    "        stats.write(\",\".join(list(map(str, [corpus, train, dev, train_toks, dev_toks])))+\"\\n\")\n",
    "\n",
    "        test_dir = \"\"\n",
    "        if corpus in tests:\n",
    "            with open(\"./1.4.5.d-lemmatisation-impact/tests/\"+corpus+\".tsv\", \"w\") as f:\n",
    "                f.write(\"\\t\".join(used_tags)+\"\\n\")\n",
    "                for sentence in output_corpus[corpus][:train]:\n",
    "                    for row in sentence:\n",
    "                        f.write(\"\\t\".join([row[tag] for tag in used_tags])+\"\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            continue\n",
    "        with open(\"./1.4.5.d-lemmatisation-impact/\"+corpus+\"-train.tsv\", \"w\") as f:\n",
    "            f.write(\"\\t\".join(used_tags)+\"\\n\")\n",
    "            for sentence in output_corpus[corpus][:train]:\n",
    "                for row in sentence:\n",
    "                    f.write(\"\\t\".join([row[tag] for tag in used_tags])+\"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        with open(\"./1.4.5.d-lemmatisation-impact/\"+corpus+\"-dev.tsv\", \"w\") as f:\n",
    "            f.write(\"\\t\".join(used_tags)+\"\\n\")\n",
    "            for sentence in output_corpus[corpus][train:]:\n",
    "                for row in sentence:\n",
    "                    f.write(\"\\t\".join([row[tag] for tag in used_tags])+\"\\n\")\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4: Configuration simples + complexes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the configuration files for all files\n",
    "import json\n",
    "\n",
    "BASE_CONFIG = json.loads(\"\"\"{\n",
    "    \"modelname\": \"model-{}-\",\n",
    "    \"modelpath\": \"./models/\",\n",
    "    \"run_test\": false,\n",
    "    \"max_sent_len\": 35,\n",
    "    \"max_sents\": 1000000,\n",
    "    \"input_path\": \"./protogenie-partial/{per}train.tsv\",\n",
    "    \"dev_path\": \"./protogenie-partial/{per}dev.tsv\",\n",
    "    \"breakline_ref\": \"pos\",\n",
    "    \"breakline_data\": \"$.\",\n",
    "    \"char_max_size\": 500,\n",
    "    \"word_max_size\": 20000,\n",
    "    \"char_min_freq\": 1,\n",
    "    \"word_min_freq\": 1,\n",
    "    \"char_eos\": true,\n",
    "    \"char_bos\": true,\n",
    "    \"header\": true,\n",
    "    \"sep\": \"\\\\t\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"name\": \"lemma\",\n",
    "            \"target\": true,\n",
    "            \"context\": \"sentence\",\n",
    "            \"level\": \"char\",\n",
    "            \"decoder\": \"attentional\",\n",
    "            \"settings\": {\n",
    "                \"bos\": true,\n",
    "                \"eos\": true,\n",
    "                \"lower\": true,\n",
    "                \"target\": \"lemma\"\n",
    "            },\n",
    "            \"layer\": -1\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"pos\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Gend\"\n",
    "        }\n",
    "    ],\n",
    "    \"task_defaults\": {\n",
    "        \"level\": \"token\",\n",
    "        \"layer\": -1,\n",
    "        \"decoder\": \"linear\",\n",
    "        \"context\": \"sentence\"\n",
    "    },\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout\": 0.25,\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_factor\": 0.5,\n",
    "    \"lr_patience\": 10,\n",
    "    \"patience\": 8,\n",
    "    \"factor\": 0.5,\n",
    "    \"threshold\": 0.0001,\n",
    "    \"min_weight\": 0.2,\n",
    "    \"include_lm\": true,\n",
    "    \"lm_shared_softmax\": true,\n",
    "    \"lm_schedule\": {\n",
    "        \"patience\": 2,\n",
    "        \"factor\": 0.5,\n",
    "        \"weight\": 0.2,\n",
    "        \"mode\": \"min\"\n",
    "    },\n",
    "    \"epochs\": 100,\n",
    "    \"cell\": \"GRU\",\n",
    "    \"num_layers\": 1,\n",
    "    \"hidden_size\": 128,\n",
    "    \"wemb_dim\": 100,\n",
    "    \"cemb_dim\": 300,\n",
    "    \"cemb_type\": \"rnn\",\n",
    "    \"cemb_layers\": 2,\n",
    "    \"checks_per_epoch\": 1,\n",
    "    \"report_freq\": 200,\n",
    "    \"verbose\": true,\n",
    "    \"device\": \"cuda\",\n",
    "    \"buffer_size\": 10000,\n",
    "    \"minimize_pad\": false,\n",
    "    \"word_dropout\": 0,\n",
    "    \"shuffle\": true,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"clip_norm\": 5,\n",
    "    \"pretrain_embeddings\": false,\n",
    "    \"load_pretrained_embeddings\": \"\",\n",
    "    \"load_pretrained_encoder\": \"\",\n",
    "    \"freeze_embeddings\": false,\n",
    "    \"custom_cemb_cell\": false,\n",
    "    \"merge_type\": \"concat\",\n",
    "    \"scorer\": \"general\",\n",
    "    \"linear_layers\": 1\n",
    "}\"\"\")\n",
    "\n",
    "import copy\n",
    "for corpus in output_corpus:\n",
    "    if corpus in tests:\n",
    "        continue\n",
    "    config = copy.deepcopy(BASE_CONFIG)\n",
    "    config[\"modelname\"] = config[\"modelname\"].format(\"1.4.5.d-\"+corpus.replace(\"+\", \"_\")+\"-\")\n",
    "    config[\"input_path\"] = \"./1.4.5.d-lemmatisation-impact/\"+corpus+\"-train.tsv\"\n",
    "    config[\"dev_path\"] = \"./1.4.5.d-lemmatisation-impact/\"+corpus+\"-dev.tsv\"\n",
    "    with open(\"configs/1.4.5.4-{}.json\".format(corpus), \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the configuration files for all files\n",
    "import json\n",
    "\n",
    "BASE_CONFIG = json.loads(\"\"\"{\n",
    "    \"modelname\": \"model-{}-\",\n",
    "    \"modelpath\": \"./models/\",\n",
    "    \"run_test\": false,\n",
    "    \"max_sent_len\": 35,\n",
    "    \"max_sents\": 1000000,\n",
    "    \"input_path\": \"./protogenie-partial/{per}train.tsv\",\n",
    "    \"dev_path\": \"./protogenie-partial/{per}dev.tsv\",\n",
    "    \"breakline_ref\": \"pos\",\n",
    "    \"breakline_data\": \"$.\",\n",
    "    \"char_max_size\": 500,\n",
    "    \"word_max_size\": 20000,\n",
    "    \"char_min_freq\": 1,\n",
    "    \"word_min_freq\": 1,\n",
    "    \"char_eos\": true,\n",
    "    \"char_bos\": true,\n",
    "    \"header\": true,\n",
    "    \"sep\": \"\\\\t\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"name\": \"lemma\",\n",
    "            \"target\": true,\n",
    "            \"context\": \"sentence\",\n",
    "            \"level\": \"char\",\n",
    "            \"decoder\": \"attentional\",\n",
    "            \"settings\": {\n",
    "                \"bos\": true,\n",
    "                \"eos\": true,\n",
    "                \"lower\": true,\n",
    "                \"target\": \"lemma\"\n",
    "            },\n",
    "            \"layer\": -1\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"pos\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Gend\"\n",
    "        }\n",
    "    ],\n",
    "    \"task_defaults\": {\n",
    "        \"level\": \"token\",\n",
    "        \"layer\": -1,\n",
    "        \"decoder\": \"linear\",\n",
    "        \"context\": \"sentence\"\n",
    "    },\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout\": 0.25,\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_factor\": 0.5,\n",
    "    \"lr_patience\": 10,\n",
    "    \"patience\": 8,\n",
    "    \"factor\": 0.5,\n",
    "    \"threshold\": 0.0001,\n",
    "    \"min_weight\": 0.2,\n",
    "    \"include_lm\": true,\n",
    "    \"lm_shared_softmax\": true,\n",
    "    \"lm_schedule\": {\n",
    "        \"patience\": 2,\n",
    "        \"factor\": 0.5,\n",
    "        \"weight\": 0.2,\n",
    "        \"mode\": \"min\"\n",
    "    },\n",
    "    \"epochs\": 100,\n",
    "    \"cell\": \"GRU\",\n",
    "    \"num_layers\": 1,\n",
    "    \"hidden_size\": 128,\n",
    "    \"wemb_dim\": 100,\n",
    "    \"cemb_dim\": 150,\n",
    "    \"cemb_type\": \"rnn\",\n",
    "    \"cemb_layers\": 1,\n",
    "    \"checks_per_epoch\": 1,\n",
    "    \"report_freq\": 200,\n",
    "    \"verbose\": true,\n",
    "    \"device\": \"cuda\",\n",
    "    \"buffer_size\": 10000,\n",
    "    \"minimize_pad\": false,\n",
    "    \"word_dropout\": 0,\n",
    "    \"shuffle\": true,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"clip_norm\": 5,\n",
    "    \"pretrain_embeddings\": false,\n",
    "    \"load_pretrained_embeddings\": \"\",\n",
    "    \"load_pretrained_encoder\": \"\",\n",
    "    \"freeze_embeddings\": false,\n",
    "    \"custom_cemb_cell\": false,\n",
    "    \"merge_type\": \"concat\",\n",
    "    \"scorer\": \"general\",\n",
    "    \"linear_layers\": 1\n",
    "}\"\"\")\n",
    "\n",
    "import copy\n",
    "for corpus in output_corpus:\n",
    "    if corpus in tests:\n",
    "        continue\n",
    "    config = copy.deepcopy(BASE_CONFIG)\n",
    "    config[\"modelname\"] = config[\"modelname\"].format(\"1.4.5.d-\"+corpus.replace(\"+\", \"_\")+\"-light-\")\n",
    "    config[\"input_path\"] = \"./1.4.5.d-lemmatisation-impact/\"+corpus+\"-train.tsv\"\n",
    "    config[\"dev_path\"] = \"./1.4.5.d-lemmatisation-impact/\"+corpus+\"-dev.tsv\"\n",
    "    with open(\"configs/1.4.5.4-{}-light.json\".format(corpus), \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
