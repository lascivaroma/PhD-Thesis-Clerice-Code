{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be run with python -m pie.scripts.evaluate\n",
    "import os.path\n",
    "from pie import utils\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "from pie.models import BaseModel\n",
    "from pie.data import Dataset, Reader\n",
    "from pie.settings import load_default_settings, settings_from_file\n",
    "from pie.tagger import Tagger\n",
    "from pie.utils import model_spec\n",
    "\n",
    "\n",
    "\n",
    "def run(model_path, test_path, train_path=None, # data\n",
    "        # decoding\n",
    "        settings_path=None, batch_size=16, buffer_size=100000, use_beam=False, beam_width=2, device=\"cuda\", \n",
    "        main_setting_task = None):  # report\n",
    "    \n",
    "    scorers = {}\n",
    "    \n",
    "    for model_no, (model, tasks) in enumerate(model_spec(model_path)):\n",
    "        print(\" - model: {}\".format(model))\n",
    "        print(\" - tasks: {}\".format(\", \".join(tasks)))\n",
    "        \n",
    "        model = BaseModel.load(model).to(device)\n",
    "        # settings\n",
    "        if hasattr(model, '_settings'):  # new models should all have _settings\n",
    "            settings = model._settings\n",
    "        elif settings_path:\n",
    "            print(\"- Using user specified settings file: {}\".format(settings_path))\n",
    "            with utils.shutup():\n",
    "                settings = settings_from_file(settings_path)\n",
    "        else:\n",
    "            print(\"- Warning! Using default settings\")\n",
    "            with utils.shutup():\n",
    "                settings = load_default_settings()\n",
    "        \n",
    "        settings.batch_size = batch_size\n",
    "        settings.buffer_size = buffer_size\n",
    "        settings.device = device\n",
    "        settings.shuffle = False    # avoid shuffling\n",
    "\n",
    "        # read datasets\n",
    "        if train_path:\n",
    "            trainset = Dataset(settings, Reader(settings, train_path), model.label_encoder)\n",
    "        elif hasattr(settings, \"input_path\") and \\\n",
    "             settings.input_path and os.path.exists(settings.input_path):\n",
    "            print(\"--- Using train set from settings\")\n",
    "            trainset = Dataset(\n",
    "                settings, Reader(settings, settings.input_path), model.label_encoder)\n",
    "        else:\n",
    "            print(\"--- Not using trainset to evaluate known/unknown tokens\")\n",
    "\n",
    "        if not len(test_path) and hasattr(settings, \"test_path\"):\n",
    "            print(\"--- Using test set from settings\")\n",
    "            test_path = (settings.test_path, )\n",
    "\n",
    "        testset = Dataset(settings, Reader(settings, *test_path), model.label_encoder)\n",
    "\n",
    "        scorers.update(model.evaluate(testset, trainset=trainset))\n",
    "        \n",
    "    return scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - model: /home/thibault/dev/latin-lasla-models/Case.tar\n",
      " - tasks: Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "643it [00:11, 57.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - model: /home/thibault/dev/latin-lasla-models/POS.tar\n",
      " - tasks: pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "643it [00:10, 58.92it/s] \n"
     ]
    }
   ],
   "source": [
    "results, tasks = run(\n",
    "    model_path=\"</home/thibault/dev/latin-lasla-models/Case.tar,Case></home/thibault/dev/latin-lasla-models/POS.tar,pos>\",\n",
    "    test_path=[\"/home/thibault/dev/thesis-lasla-pipeline/test.tsv\"],\n",
    "    train_path=\"/home/thibault/dev/thesis-lasla-pipeline/train.tsv\"\n",
    "    #test_path=[\"/home/thibault/dev/thesis-lasla-pipeline/mood-tense-voice-pft-clitics-uppercase/test.tsv\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/dev/these/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task      accuracy    macr-pre    macr-rec    macr-f1    weig-pre    weig-rec    weig-f1\n",
      "------  ----------  ----------  ----------  ---------  ----------  ----------  ---------\n",
      "Case         94.64       90.38       88.82      89.56       94.63       94.64      94.63\n",
      "pos          96.97       88.51       88.09      88.27       96.96       96.97      96.96\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"def dict(scorer):\n",
    "    return {\n",
    "        \"known_tokens\": self.known_tokens,\n",
    "        \"amb_tokens\": self.amb_tokens,\n",
    "        \"preds\": self.preds,\n",
    "        \"trues\": self.trues,\n",
    "        \"tokens\": self.tokens\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, balanced_accuracy_score\n",
    "\n",
    "elements = []\n",
    "\n",
    "def normalize_nom(value):\n",
    "    if value[:3] == \"NOM\":\n",
    "        return \"NOM\"\n",
    "    return value\n",
    "CLEAN_POS = True\n",
    "\n",
    "for task, scorer in results.items():\n",
    "    # (y_true, y_pred,\n",
    "    if task == \"pos\" and CLEAN_POS is True:\n",
    "        data = classification_report(\n",
    "            results[task].trues,\n",
    "            [normalize_nom(x) for x in results[task].preds],\n",
    "            output_dict=True\n",
    "        )\n",
    "    else:\n",
    "        data = classification_report(results[task].trues, results[task].preds, output_dict=True)\n",
    "    elements.append({\n",
    "        **{\"task\": task,\n",
    "         \"accuracy\": data[\"accuracy\"]*100}, \n",
    "        **{\n",
    "            f\"{key[:4]}-{category[:3].strip('-')}\": value*100\n",
    "            for key in ('macro avg', \"weighted avg\")\n",
    "            for category, value in data[key].items()\n",
    "            if category != \"support\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "from tabulate import tabulate\n",
    "print(tabulate(elements, floatfmt=\".2f\", headers=\"keys\"))\n",
    "\n",
    "# Précision: \n",
    "#  quand un lemme est choisi, on compare les bonnes réponses aux réponses bonnes et mauvaises pour tel lemme\n",
    "# Rappel\n",
    "#  Quand un lemme est choisi, on compare les bonnes réponses aux bonnes réponses + bonnes réponses manquantes\n",
    "# Macro:\n",
    "#  Calculate metrics for each label, and find their unweighted mean. \n",
    "#      This does not take label imbalance into account.\n",
    "#  -> Sur-représente de potentielles rares ou uniques formes.\n",
    "# Weighted:\n",
    "#  Calculate metrics for each label, and find their average weighted by support \n",
    "#   -> Avantage les formes très courantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "full_results: List[bool] = [\n",
    "    True for _ in range(len(results[\"lemma\"].preds))    \n",
    "]\n",
    "full_results_unknown: List[bool] = [   \n",
    "]\n",
    "lemma = []\n",
    "\n",
    "tasks = {\n",
    "    task: []\n",
    "    for task in results if task not in (\"Dis\", \"Entity\", \"lemma\")\n",
    "}\n",
    "\n",
    "known_tokens = set(results[\"lemma\"].known_tokens)\n",
    "for task, scorer in results.items():\n",
    "    if task in (\"Dis\", \"Entity\"):\n",
    "        #print(scorer.__dict__.keys())\n",
    "        continue\n",
    "        \n",
    "    for idx, (p, t, cur, tok) in enumerate(zip(scorer.preds, scorer.trues, full_results, scorer.tokens)):\n",
    "        full_results[idx] = cur and p == t\n",
    "        \n",
    "        if task == \"lemma\":\n",
    "            lemma.append(p == t)\n",
    "        elif t != \"_\":\n",
    "            tasks[task].append(p == t)\n",
    "        if tok not in known_tokens:\n",
    "            full_results_unknown.append(full_results[idx])\n",
    "    # scorer.print_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\hline\n",
      " Mesure                             &   Score \\\\\n",
      "\\hline\n",
      " Tâches Agrégées                    &   61.72 \\\\\n",
      " Tâches Agrégées (Formes inconnues) &   61.97 \\\\\n",
      " pos                                &   68.38 \\\\\n",
      " Gend                               &   89.95 \\\\\n",
      " Numb                               &   96.74 \\\\\n",
      " Case                               &   87.69 \\\\\n",
      " Deg                                &   92.54 \\\\\n",
      " Mood\\_Tense\\_Voice                   &   94.26 \\\\\n",
      " Person                             &   98.62 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "table = [\n",
    "    (\"Mesure\", \"Score\")\n",
    "]\n",
    "#data.append((\"Lemme\", f\"{lemma.count(True)*100/len(lemma):.2f}\"))\n",
    "table.append((\"Tâches Agrégées\", f\"{full_results.count(True)*100/len(full_results):.2f}\"))\n",
    "table.append((\"Tâches Agrégées (Formes inconnues)\",\n",
    "             f\"{full_results_unknown.count(True)*100/len(full_results_unknown):.2f}\"))\n",
    "for task, task_score in tasks.items():\n",
    "    table.append((f\"{task}\", f\"{task_score.count(True)*100/len(task_score):.2f}\"))\n",
    "    \n",
    "print(tabulate(table[1:], headers=table[0], tablefmt=\"latex\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
