{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "Montrer l'impact du pre-processing.\n",
    "\n",
    "## Methodes\n",
    "\n",
    "1. Comparaison de l'annotation via normalisation et segmentation de phrases simple\n",
    "2. Comparaison du nombre de tokens par analyse via Pie vs Pie-Extended et résultat ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "TASKS = \"lemma,Deg,Numb,Person,Mood_Tense_Voice,Case,Gend,pos\".split(\",\")\n",
    "\n",
    "\n",
    "SENTENCE_CHAR_LENGTH = 35\n",
    "\n",
    "def pretty_print(sentences, size=5):\n",
    "    df = pandas.DataFrame([\n",
    "        {task: tok.get(task, \"_\") for task in [\"form\"]+TASKS}\n",
    "        for sent in sentences[:size] \n",
    "        for tok in sent+[{\"form\": \"SENTENCE_BREAK\"}]\n",
    "    ], dtype=str)\n",
    "    print(df.to_string())\n",
    "    \n",
    "PrettyName = {\n",
    "    \"Normalized\": \"Prétraitement complet\",\n",
    "    \"Raw\": \"Prétraitement absent, tokenization phrase conservé\",\n",
    "    \"RawVJUI\": \"Prétraitement absent, VJUI OUI, tokenization phrase conservé\",\n",
    "    \"Chunks35\": f\"Prétraitement absent, tokenization tous les {SENTENCE_CHAR_LENGTH} caractères\",\n",
    "    \"Chunks35Normalized\": f\"Prétraitement complet, tokenization tous les {SENTENCE_CHAR_LENGTH} caractères\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données\n",
    "\n",
    "### Imports généraux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from tools.analysis_utils import (\n",
    "    import_known_tokens,\n",
    "    compile_scores,\n",
    "    convert_raw,\n",
    "    vjui\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsage du document\n",
    "\n",
    "Pour chaque document, on conserve:\n",
    "\n",
    "1. Une version normalisée, où l'on supprime la ponctuation et l'on tokenise via les points forts.\n",
    "2. Une version non-normalisée, où l'on conserve la ponctuation et l'on tokenise via les points forts.\n",
    "3. Une version non-normalisée où la tokenisation est faite à chaque 35eme mot (défaut de Pie)\n",
    "3. Une version normalisée où la tokenisation est faite à chaque 35eme mot (défaut de Pie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "\t716 sentences found\n",
      "\t15367 tokens found\n",
      "Raw\n",
      "\t717 sentences found\n",
      "\t18138 tokens found\n",
      "RawVJUI\n",
      "\t717 sentences found\n",
      "\t18138 tokens found\n",
      "Chunks35\n",
      "\t527 sentences found\n",
      "\t18138 tokens found\n",
      "Chunks35Normalized\n",
      "\t448 sentences found\n",
      "\t15367 tokens found\n"
     ]
    }
   ],
   "source": [
    "Texts = []\n",
    "\n",
    "# Text normalized\n",
    "Normalized = [[]]\n",
    "# Chunked using punctuation but no modification\n",
    "Raw = [[]]\n",
    "# No modification, chunk = 35 characters\n",
    "Chunks35 = [[]]\n",
    "# Normalization, chunk = 35 characters\n",
    "Chunks35Normalized = [[]]\n",
    "# Chunked with punctuation, VJUI applied\n",
    "RawVJUI = [[]]\n",
    "\n",
    "Versions = {\n",
    "    \"Normalized\": Normalized,\n",
    "    \"Raw\": Raw,\n",
    "    \"RawVJUI\": RawVJUI,\n",
    "    \"Chunks35\": Chunks35,\n",
    "    \"Chunks35Normalized\": Chunks35Normalized\n",
    "}\n",
    "\n",
    "EndToken = 14225 # As per Anthony's sheet\n",
    "nb_tokens = 0\n",
    "\n",
    "\n",
    "def filter_gold(data):\n",
    "    return [lst for lst in data if lst]\n",
    "\n",
    "def append(val, *arrs):\n",
    "    for x in arrs:\n",
    "        x[-1].append(val)\n",
    "        \n",
    "def create_chunks(*arrs):\n",
    "    for x in filter_gold(arrs):\n",
    "        if x[-1] != []:\n",
    "            x.append([])\n",
    "        yield x\n",
    "\n",
    "def filter_all(*arrs):\n",
    "    for x in arrs:\n",
    "        yield filter_gold(x)\n",
    "\n",
    "def count_toks(sentences):\n",
    "    return len([tok for sent in sentences for tok in sent])\n",
    "        \n",
    "\n",
    "TextsMilestonesStart = {\n",
    "    \n",
    "}\n",
    "\n",
    "HARD_PUNCT = \"?.!\"\n",
    "\n",
    "\n",
    "Milestones = set(list(range(0, 14226)) + list(range(20384, 24316)))\n",
    "\n",
    "with open(\"/home/thibault/dev/latin-non-classical-data/latin-chretien-v2.tsv\") as f:\n",
    "    header = []\n",
    "    current_text = None\n",
    "    last_token = {}\n",
    "    for lineno, line in enumerate(f):\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        if lineno == 0:\n",
    "            header = line\n",
    "            continue\n",
    "        line = dict(zip(header, line))\n",
    "        \n",
    "        if lineno not in Milestones:\n",
    "            continue\n",
    "        \n",
    "        if line[\"form\"].startswith(\"urn:\"):\n",
    "            TextsMilestonesStart[line[\"form\"]] = {\n",
    "                version: (len(arr) - 1, count_toks(arr))\n",
    "                for version, arr in Versions.items()\n",
    "            }\n",
    "            last_token = {}\n",
    "            Raw, Chunks35, Normalized, Chunks35Normalized, RawVJUI = create_chunks(\n",
    "                Raw, Chunks35, Normalized, Chunks35Normalized, RawVJUI\n",
    "            ) \n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if line[\"POS\"] == \"PUNC\":\n",
    "            append(line, Raw, Chunks35, RawVJUI)\n",
    "            if line[\"lemma\"] in HARD_PUNCT and last_token.get(\"lemma\") not in \".!?\":\n",
    "                Normalized, Raw, RawVJUI = create_chunks(Normalized, Raw, RawVJUI)\n",
    "        else:\n",
    "            append(line, Raw, Chunks35, Normalized, Chunks35Normalized, RawVJUI)\n",
    "        \n",
    "        if len(Chunks35[-1]) == SENTENCE_CHAR_LENGTH:\n",
    "            Chunks35, *_ = create_chunks(Chunks35)\n",
    "        if len(Chunks35Normalized[-1]) == SENTENCE_CHAR_LENGTH:\n",
    "            Chunks35Normalized, *_ = create_chunks(Chunks35Normalized)\n",
    "        \n",
    "        last_token = line\n",
    "        #if lineno+1 == EndToken:\n",
    "        #    break   \n",
    "\n",
    "Normalized, Raw, Chunks35, Chunks35Normalized, RawVJUI = filter_all(\n",
    "    Normalized, Raw, Chunks35, Chunks35Normalized, RawVJUI\n",
    ")\n",
    "\n",
    "for version, arr in Versions.items():\n",
    "    print(f\"{version}\")\n",
    "    print(f\"\\t{len(arr)} sentences found\")\n",
    "    print(f\"\\t{count_toks(arr)} tokens found\")\n",
    "# print(TextsMilestonesStart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "Pour chacune des version, on applique des fonctions de normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:00<00:00, 14726.17it/s]\n",
      "100%|██████████| 717/717 [00:00<00:00, 12283.08it/s]\n",
      "100%|██████████| 717/717 [00:00<00:00, 11039.14it/s]\n",
      "100%|██████████| 527/527 [00:00<00:00, 9441.75it/s]\n",
      "100%|██████████| 448/448 [00:00<00:00, 8309.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               form        lemma   Deg  Numb Person Mood_Tense_Voice Case      Gend     pos\n",
      "0            Mutant         muto     _  Plur      3     Ind|Pres|Act    _         _     VER\n",
      "1                et           et     _     _      _                _    _         _  CONcoo\n",
      "2           bestiae       bestia     _  Plur      _                _  Nom         _  NOMcom\n",
      "3               pro          pro     _     _      _                _    _         _     PRE\n",
      "4             veste       uestis     _  Sing      _                _  Abl         _  NOMcom\n",
      "5            formam        forma     _  Sing      _                _  Acc         _  NOMcom\n",
      "6                 ;            ;     _     _      _                _    _         _    PUNC\n",
      "7          quamquam     quamquam     _     _      _                _    _         _  CONsub\n",
      "8                et           et     _     _      _                _    _         _  CONcoo\n",
      "9              pavo        pauus     _  Sing      _                _  Dat         _  NOMcom\n",
      "10            pluma        pluma     _  Sing      _                _  Nom         _  NOMcom\n",
      "11           vestis       uestis     _  Sing      _                _  Nom         _  NOMcom\n",
      "12                ,            ,     _     _      _                _    _         _    PUNC\n",
      "13               et           et     _     _      _                _    _         _  CONcoo\n",
      "14           quidem       quidem   Pos     _      _                _    _         _     ADV\n",
      "15               de           de     _     _      _                _    _         _     PRE\n",
      "16      cataclistis    cataclita     _  Plur      _                _  Abl         _  NOMcom\n",
      "17                ,            ,     _     _      _                _    _         _    PUNC\n",
      "18             immo         immo   Pos     _      _                _    _         _     ADV\n",
      "19             omni        omnis     _  Sing      _                _  Abl       Com  PROind\n",
      "20        conchylio   conchylium     _  Sing      _                _  Abl         _  NOMcom\n",
      "21         pressior      pressus  Comp  Sing      _                _  Nom   MascFem  ADJqua\n",
      "22              qua          qua     _     _      _                _    _         _  ADVrel\n",
      "23            colla       collum     _  Plur      _                _  Nom         _  NOMcom\n",
      "24          florent       floreo     _  Plur      3     Ind|Pres|Act    _         _     VER\n",
      "25                ,            ,     _     _      _                _    _         _    PUNC\n",
      "26               et           et     _     _      _                _    _         _  CONcoo\n",
      "27             omni        omnis     _  Sing      _                _  Abl       Com  PROind\n",
      "28          patagio     patagium     _  Sing      _                _  Abl         _  NOMcom\n",
      "29       inauratior    inauratus  Comp  Sing      _                _  Nom   MascFem  ADJqua\n",
      "30              qua          qua     _     _      _                _    _         _  ADVrel\n",
      "31            terga       tergum     _  Plur      _                _  Nom         _  NOMcom\n",
      "32          fulgent       fulgeo     _  Plur      3     Ind|Pres|Act    _         _     VER\n",
      "33                ,            ,     _     _      _                _    _         _    PUNC\n",
      "34               et           et     _     _      _                _    _         _  CONcoo\n",
      "35             omni        omnis     _  Sing      _                _  Abl       Com  PROind\n",
      "36          syrmate        syrma     _  Sing      _                _  Abl         _  NOMcom\n",
      "37         solutior      solutus  Comp  Sing      _                _  Nom   MascFem  ADJqua\n",
      "38              qua          qua     _     _      _                _    _         _  ADVrel\n",
      "39           caudae        cauda     _  Plur      _                _  Nom         _  NOMcom\n",
      "40           iacent        iaceo     _  Plur      3     Ind|Pres|Act    _         _     VER\n",
      "41                ,            ,     _     _      _                _    _         _    PUNC\n",
      "42       multicolor   multicolor   Pos  Sing      _                _  Nom       Com  ADJqua\n",
      "43               et           et     _     _      _                _    _         _  CONcoo\n",
      "44         discolor     discolor   Pos  Sing      _                _  Nom       Com  ADJqua\n",
      "45               et           et     _     _      _                _    _         _  CONcoo\n",
      "46       versicolor   uersicolor   Pos  Sing      _                _  Nom       Com  ADJqua\n",
      "47                ,            ,     _     _      _                _    _         _    PUNC\n",
      "48          nunquam      numquam     _     _      _                _    _         _  ADVneg\n",
      "49             ipsa         ipse     _  Sing      _                _  Nom       Fem  PROdem\n",
      "50           semper       semper   Pos     _      _                _    _         _     ADV\n",
      "51             alia        alius     _  Sing      _                _  Nom       Fem  PROind\n",
      "52                ,            ,     _     _      _                _    _         _    PUNC\n",
      "53             etsi         etsi     _     _      _                _    _         _  CONsub\n",
      "54           semper       semper   Pos     _      _                _    _         _     ADV\n",
      "55             ipsa         ipse     _  Sing      _                _  Nom       Fem  PROdem\n",
      "56           quando       quando     _     _      _                _    _         _  CONsub\n",
      "57             alia        alius     _  Sing      _                _  Nom       Fem  PROind\n",
      "58                ,            ,     _     _      _                _    _         _    PUNC\n",
      "59          totiens      totiens   Pos     _      _                _    _         _     ADV\n",
      "60          denique      denique   Pos     _      _                _    _         _     ADV\n",
      "61          mutanda         muto     _  Sing      _       Adj|_|Pass  Nom       Fem     VER\n",
      "62         quotiens     quotiens     _     _      _                _    _         _  ADVrel\n",
      "63          movenda        moueo     _  Sing      _       Adj|_|Pass  Nom       Fem     VER\n",
      "64                .            .     _     _      _                _    _         _    PUNC\n",
      "65   SENTENCE_BREAK            _     _     _      _                _    _         _       _\n",
      "66       Nominandus       nomino     _  Sing      _       Adj|_|Pass  Nom      Masc     VER\n",
      "67              est          sum     _  Sing      3     Ind|Pres|Act    _         _     VER\n",
      "68               et           et     _     _      _                _    _         _  CONcoo\n",
      "69          serpens      serpens     _  Sing      _                _  Nom         _  NOMcom\n",
      "70                ,            ,     _     _      _                _    _         _    PUNC\n",
      "71            licet        licet   Pos     _      _                _    _         _     ADV\n",
      "72             pone         pono     _  Sing      2     Imp|Pres|Act    _         _     VER\n",
      "73            pavum        pauus     _  Sing      _                _  Acc         _  NOMcom\n",
      "74                ;            ;     _     _      _                _    _         _    PUNC\n",
      "75              nam          nam     _     _      _                _    _         _  CONcoo\n",
      "76                ?            ?     _     _      _                _    _         _    PUNC\n",
      "77   SENTENCE_BREAK            _     _     _      _                _    _         _       _\n",
      "78                ?            ?     _     _      _                _    _         _    PUNC\n",
      "79                )            )     _     _      _                _    _         _    PUNC\n",
      "80               et           et     _     _      _                _    _         _  CONcoo\n",
      "81             iste         iste     _  Sing      _                _  Nom      Masc  PROdem\n",
      "82             quod          qui     _  Sing      _                _  Nom      Neut  PROrel\n",
      "83         sortitus      sortior     _  Sing      _     Par|Perf|Dep  Nom      Masc     VER\n",
      "84              est          sum     _  Sing      3     Ind|Pres|Act    _         _  VERaux\n",
      "85        convertit     conuerto     _  Sing      3     Ind|Pres|Act    _         _     VER\n",
      "86                ,            ,     _     _      _                _    _         _    PUNC\n",
      "87           corium       corium     _  Sing      _                _  Acc         _  NOMcom\n",
      "88               et           et     _     _      _                _    _         _  CONcoo\n",
      "89            aevum        aeuum     _  Sing      _                _  Acc         _  NOMcom\n",
      "90                .            .     _     _      _                _    _         _    PUNC\n",
      "91   SENTENCE_BREAK            _     _     _      _                _    _         _       _\n",
      "92         Siquidem     siquidem     _     _      _                _    _         _  CONsub\n",
      "93               ut           ut     _     _      _                _    _         _  CONsub\n",
      "94           senium       senium     _  Sing      _                _  Acc         _  NOMcom\n",
      "95        persensit    persentio     _  Sing      3     Ind|Perf|Act    _         _     VER\n",
      "96                ,            ,     _     _      _                _    _         _    PUNC\n",
      "97               in           in     _     _      _                _    _         _     PRE\n",
      "98        angustias     angustia     _  Plur      _                _  Acc         _  NOMcom\n",
      "99           stipat        stipo     _  Sing      3     Ind|Pres|Act    _         _     VER\n",
      "100               ,            ,     _     _      _                _    _         _    PUNC\n",
      "101      pariterque  pariter界que   Pos     _      _                _    _         _     ADV\n",
      "102          specum       specus     _  Sing      _                _  Acc         _  NOMcom\n",
      "103      ingrediens    ingredior     _  Sing      _     Par|Pres|Dep  Nom      Masc     VER\n",
      "104              et           et     _     _      _                _    _         _  CONcoo\n",
      "105           cutem        cutis     _  Sing      _                _  Acc         _  NOMcom\n",
      "106       egrediens     egredior     _  Sing      _     Par|Pres|Dep  Nom      Masc     VER\n",
      "107              ab           ab     _     _      _                _    _         _     PRE\n",
      "108            ipso         ipse     _  Sing      _                _  Abl  MascNeut  PROdem\n",
      "109          statim       statim   Pos     _      _                _    _         _     ADV\n",
      "110          limine        limen     _  Sing      _                _  Abl         _  NOMcom\n",
      "111          erasus        erado     _  Sing      _    Par|Perf|Pass  Nom      Masc     VER\n",
      "112         exuviis      exuuiae     _  Plur      _                _  Abl         _  NOMcom\n",
      "113          ibidem       ibidem   Pos     _      _                _    _         _     ADV\n",
      "114        relictis     relinquo     _  Plur      _    Par|Perf|Pass  Abl       Fem     VER\n",
      "115           novus        nouus   Pos  Sing      _                _  Nom      Masc  ADJqua\n",
      "116        explicat      explico     _  Sing      3     Ind|Pres|Act    _         _     VER\n",
      "117               ;            ;     _     _      _                _    _         _    PUNC\n",
      "118             cum          cum     _     _      _                _    _         _     PRE\n",
      "119         squamis       squama     _  Plur      _                _  Abl         _  NOMcom\n",
      "120              et           et     _     _      _                _    _         _  CONcoo\n",
      "121            anni        annus     _  Plur      _                _  Nom         _  NOMcom\n",
      "122      recusantur       recuso     _  Plur      3    Ind|Pres|Pass    _         _     VER\n",
      "123               .            .     _     _      _                _    _         _    PUNC\n",
      "124  SENTENCE_BREAK            _     _     _      _                _    _         _       _\n",
      "125         Hyaenam       hyaena     _  Sing      _                _  Acc         _  NOMcom\n",
      "126              si           si     _     _      _                _    _         _  CONsub\n",
      "127        observes      obseruo     _  Sing      2     Sub|Pres|Act    _         _     VER\n",
      "128               ,            ,     _     _      _                _    _         _    PUNC\n",
      "129           sexus        sexus     _  Sing      _                _  Nom         _  NOMcom\n",
      "130         annalis      annalis   Pos  Sing      _                _  Nom      Masc  ADJqua\n",
      "131             est          sum     _  Sing      3     Ind|Pres|Act    _         _     VER\n",
      "132               ,            ,     _     _      _                _    _         _    PUNC\n",
      "133           marem          mas     _  Sing      _                _  Acc         _  NOMcom\n",
      "134              et           et     _     _      _                _    _         _  CONcoo\n",
      "135         feminam       femina     _  Sing      _                _  Acc         _  NOMcom\n",
      "136        alternat      alterno     _  Sing      3     Sub|Pres|Act    _         _     VER\n",
      "137               .            .     _     _      _                _    _         _    PUNC\n",
      "138  SENTENCE_BREAK            _     _     _      _                _    _         _       _\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Normalisations = {\n",
    "    \"Normalized\": vjui,\n",
    "    \"Chunks35Normalized\": vjui,\n",
    "    \"RawVJUI\": vjui,\n",
    "    \"Raw\": lambda x: x,\n",
    "    \"Chunks35\": lambda x: x\n",
    "}\n",
    "\n",
    "PlatinumVersions = {\n",
    "    version: convert_raw(sentences, task_list=TASKS, lemma_fn=vjui, form_fn=Normalisations[version], \n",
    "                       pos_fn=lambda x:x)\n",
    "    for version, sentences in Versions.items()\n",
    "}\n",
    "pretty_print(PlatinumVersions[\"Raw\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mutant', 'et', 'bestiae', 'pro', 'veste', 'formam', ';', 'quamquam', 'et', 'pavo', 'pluma', 'vestis', ',', 'et', 'quidem', 'de', 'cataclistis', ',', 'immo', 'omni', 'conchylio', 'pressior', 'qua', 'colla', 'florent', ',', 'et', 'omni', 'patagio', 'inauratior', 'qua', 'terga', 'fulgent', ',', 'et']\n"
     ]
    }
   ],
   "source": [
    "TokensVersion = {\n",
    "    version: [\n",
    "        [x[\"form\"] for x in sentence]\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "    for version, sentences in PlatinumVersions.items()\n",
    "}\n",
    "print(TokensVersion[\"Chunks35\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging : `Normalized`\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:13,  1.70s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging : `Raw`\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:15,  1.88s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging : `RawVJUI`\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:14,  1.85s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging : `Chunks35`\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:13,  2.31s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging : `Chunks35Normalized`\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:12,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "from pie.tagger import Tagger\n",
    "from pie.utils import chunks\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "OutputVersion = {\n",
    "    version: []\n",
    "    for version in TokensVersion\n",
    "}\n",
    "\n",
    "# MODEL_PATH = \"../../../pie/models/FinalModelVulgate-lemma-2020_10_14-18_09_35.tar\"\n",
    "MODEL_PATH = \"../../../latin-lasla-models/model.tar\"\n",
    "\n",
    "tagger = Tagger(device=\"cpu\", batch_size=100, lower=False, tokenize=False)\n",
    "tagger.add_model(MODEL_PATH, *TASKS)\n",
    "\n",
    "for version, sentences in TokensVersion.items():\n",
    "    print(f\"Tagging : `{version}`\\n\")\n",
    "    for chunk in tqdm(chunks([(sent, len(sent)) for sent in sentences], tagger.batch_size)):\n",
    "        tagged, tasks = tagger.tag(*zip(*chunk))#, use_beam=True)\n",
    "        OutputVersion[version].extend([\n",
    "            [\n",
    "                (token, dict(zip(tasks, result)))\n",
    "                for token, result in sentence\n",
    "            ]\n",
    "            for sentence in tagged\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des version alignées\n",
    "\n",
    "La sortie comprend des tokens qui ne doivent pas être comptabilisées dans les scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "punkts = regex.compile(r\"\\W+\")\n",
    "greek = regex.compile(r\"\\p{Greek}+\")\n",
    "\n",
    "def keep_tokens(token):\n",
    "    if punkts.match(token):\n",
    "        #print(token)\n",
    "        return False\n",
    "    elif greek.match(token):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "CleanedUpVersions = {\n",
    "    version: [\n",
    "        [\n",
    "            (token, annots)\n",
    "            for (token, annots) in sentence\n",
    "            if keep_tokens(token)\n",
    "        ]\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "    for version, sentences in OutputVersion.items()\n",
    "    #if not print(version)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variétés des formes: richesse lexicale des lemmes\n",
    "\n",
    "Dans cette analyse, on s'intéresse au nombre de lemmes différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw has 3459 different lemma\n",
      "Raw has 3.32% more lemma than the 'Perfect' version\n",
      "\n",
      "RawVJUI has 3366 different lemma\n",
      "RawVJUI has 0.54% more lemma than the 'Perfect' version\n",
      "\n",
      "Chunks35 has 3468 different lemma\n",
      "Chunks35 has 3.58% more lemma than the 'Perfect' version\n",
      "\n",
      "Chunks35Normalized has 3355 different lemma\n",
      "Chunks35Normalized has 0.21% more lemma than the 'Perfect' version\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RichesseLexicaleLemmes = {\n",
    "    version: len(set([\n",
    "        annot[\"lemma\"]\n",
    "        for sent in sentences\n",
    "        for _, annot in sent\n",
    "    ]))\n",
    "    for version, sentences in CleanedUpVersions.items()\n",
    "}\n",
    "\n",
    "for version, richesse in RichesseLexicaleLemmes.items():\n",
    "    if version != \"Normalized\":\n",
    "        print(f\"{version} has {richesse} different lemma\")\n",
    "        print(\n",
    "            f\"{version} has {(richesse/RichesseLexicaleLemmes['Normalized']-1)*100:.2f}% \"\n",
    "            \"more lemma than the 'Perfect' version\\n\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score accuracy\n",
    "\n",
    "On calcule maintenant l'accuracy de chacune des versions, en excluant du calcul les formes interdites.\n",
    "\n",
    "Rappel: les versions de comparaison sont contenues dans `PlatinumVersions` et les prédictions dans `OutputVersion`\n",
    "\n",
    "#### Création des comparaisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores = {\n",
    "    version: {task:[] for task in TASKS}\n",
    "    for version in PlatinumVersions\n",
    "}\n",
    "for version, gt_sentences in PlatinumVersions.items():\n",
    "    # gt_sentences = [{annotation_dicts}]\n",
    "    pred_sentences = OutputVersion[version]\n",
    "    for gt_sentence, pred_sentence in zip(gt_sentences, pred_sentences):\n",
    "        # We filter out noise so we have the same basis in terms of comparison\n",
    "        for gt_annot, (token, pred_annot) in zip(gt_sentence, pred_sentence):\n",
    "            if not keep_tokens(gt_annot[\"form\"]):\n",
    "                continue\n",
    "            if token != gt_annot[\"form\"]:\n",
    "                print(\"ERROR\", version, token, gt_annot[\"form\"])\n",
    "            for task in TASKS:\n",
    "                Scores[version][task].append(int(gt_annot[task] == pred_annot[task]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculs globaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Prétraitement absent, tokenization phrase conservé  Prétraitement absent, VJUI OUI, tokenization phrase conservé  Prétraitement absent, tokenization tous les 35 caractères  Prétraitement complet, tokenization tous les 35 caractères\n",
      "lemma                                                      -1.513572                                                     -0.222446                                                  -1.624495                                                   -0.226986\n",
      "Deg                                                        -0.301302                                                     -0.157115                                                  -0.261781                                                   -0.064441\n",
      "Numb                                                       -0.412643                                                     -0.307780                                                  -0.563092                                                   -0.122956\n",
      "Person                                                     -0.045853                                                      0.000025                                                  -0.058932                                                   -0.032610\n",
      "Mood_Tense_Voice                                           -0.111265                                                     -0.019510                                                  -0.124213                                                   -0.038399\n",
      "Case                                                       -0.759774                                                     -0.661465                                                  -1.408038                                                   -0.318139\n",
      "Gend                                                       -0.281338                                                     -0.130598                                                  -0.228411                                                   -0.108495\n",
      "pos                                                        -0.568048                                                     -0.325552                                                  -0.664169                                                   -0.183689\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &  Prétraitement absent, tokenization phrase conservé &  Prétraitement absent, VJUI OUI, tokenization phrase conservé &  Prétraitement absent, tokenization tous les 35 caractères &  Prétraitement complet, tokenization tous les 35 caractères \\\\\n",
      "\\midrule\n",
      "lemma            &                                             -1.51\\% &                                             -0.22\\% &                                             -1.62\\% &                                             -0.23\\% \\\\\n",
      "Deg              &                                             -0.30\\% &                                             -0.16\\% &                                             -0.26\\% &                                             -0.06\\% \\\\\n",
      "Numb             &                                             -0.41\\% &                                             -0.31\\% &                                             -0.56\\% &                                             -0.12\\% \\\\\n",
      "Person           &                                             -0.05\\% &                                              0.00\\% &                                             -0.06\\% &                                             -0.03\\% \\\\\n",
      "Mood\\_Tense\\_Voice &                                             -0.11\\% &                                             -0.02\\% &                                             -0.12\\% &                                             -0.04\\% \\\\\n",
      "Case             &                                             -0.76\\% &                                             -0.66\\% &                                             -1.41\\% &                                             -0.32\\% \\\\\n",
      "Gend             &                                             -0.28\\% &                                             -0.13\\% &                                             -0.23\\% &                                             -0.11\\% \\\\\n",
      "pos              &                                             -0.57\\% &                                             -0.33\\% &                                             -0.66\\% &                                             -0.18\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "|                  |   Prétraitement absent, tokenization phrase conservé |   Prétraitement absent, VJUI OUI, tokenization phrase conservé |   Prétraitement absent, tokenization tous les 35 caractères |   Prétraitement complet, tokenization tous les 35 caractères |\n",
      "|:-----------------|-----------------------------------------------------:|---------------------------------------------------------------:|------------------------------------------------------------:|-------------------------------------------------------------:|\n",
      "| lemma            |                                           -1.51357   |                                                    -0.222446   |                                                  -1.62449   |                                                   -0.226986  |\n",
      "| Deg              |                                           -0.301302  |                                                    -0.157115   |                                                  -0.261781  |                                                   -0.0644409 |\n",
      "| Numb             |                                           -0.412643  |                                                    -0.30778    |                                                  -0.563092  |                                                   -0.122956  |\n",
      "| Person           |                                           -0.0458527 |                                                     2.4915e-05 |                                                  -0.0589318 |                                                   -0.0326095 |\n",
      "| Mood_Tense_Voice |                                           -0.111265  |                                                    -0.0195097  |                                                  -0.124213  |                                                   -0.0383986 |\n",
      "| Case             |                                           -0.759774  |                                                    -0.661465   |                                                  -1.40804   |                                                   -0.318139  |\n",
      "| Gend             |                                           -0.281338  |                                                    -0.130598   |                                                  -0.228411  |                                                   -0.108495  |\n",
      "| pos              |                                           -0.568048  |                                                    -0.325552   |                                                  -0.664169  |                                                   -0.183689  |\n"
     ]
    }
   ],
   "source": [
    "GlobAccuracies = {\n",
    "    version: {\n",
    "        task: sum(comparison_ints) / len(comparison_ints)*100\n",
    "        for task, comparison_ints in tasks.items()\n",
    "    }\n",
    "    for version, tasks in Scores.items()\n",
    "}\n",
    "GlobAccuraciesDF = pandas.DataFrame({\n",
    "    PrettyName[version]: {\n",
    "        task: task_score-GlobAccuracies[\"Normalized\"][task]\n",
    "        for task, task_score in tasks.items()\n",
    "    }\n",
    "    for version, tasks in GlobAccuracies.items()\n",
    "    if version != \"Normalized\"\n",
    "})\n",
    "print(GlobAccuraciesDF.to_string())\n",
    "\n",
    "print(GlobAccuraciesDF.to_latex(float_format=\"{:0.2f}%\".format, decimal=\",\"))\n",
    "\n",
    "print(GlobAccuraciesDF.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "En dehors des cas, ce qui semble logique, la tokenisation des phrases a un impact assez limité sur la lemmatisation et les tâches associées. Au contraire, la conservation de mots (principalement la ponctuation) et de lettres inconnues a un impact assez fort sur le résultat final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
