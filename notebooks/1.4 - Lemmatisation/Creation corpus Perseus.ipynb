{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un corpus équivalent au corpus Perseus Treebank\n",
    "\n",
    "\n",
    "## Décompte des tokens par oeuvres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phi0975.phi001.perseus-lat1.tb.xml', 'phi1221.phi007.perseus-lat1.tb.xml', 'phi1348.abo012.perseus-lat1.tb.xml', 'phi1351.phi005.perseus-lat1.tb.xml', 'tlg0031.tlg027.perseus-lat1.tb.xml', 'phi0448.phi001.perseus-lat1.tb.xml', 'phi0474.phi013.perseus-lat1.tb.xml', 'phi0620.phi001.perseus-lat1.tb.xml', 'phi0631.phi001.perseus-lat1.tb.xml', 'phi0690.phi003.perseus-lat1.tb.xml']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "chunks = Counter()\n",
    "tokens = Counter()\n",
    "\n",
    "with open(\"UD_Perseus_train_2.1.txt\") as f:\n",
    "    current_text = None\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(\"# sent_id = \"):\n",
    "            text_id = line[len(\"# sent_id = \"):].split(\"@\")[0]\n",
    "            chunks[text_id] += 1\n",
    "            current_text = text_id\n",
    "        elif line[0].isnumeric():\n",
    "            tokens[current_text] += 1\n",
    "            \n",
    "print(list(tokens.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur l'ensemble des textes trouvés dans le fichier d'entraînement de Universal Dependencies, 4 oeuvres ne sont pas disponibles dans le corpus du LASLA:\n",
    "\n",
    "1. Phèdres, Fables\n",
    "2. Auguste, Res Gestae\n",
    "3. Suétone Vie d'Auguste\n",
    "4. Vulgate\n",
    "\n",
    "2 oeuvres supplémentaires sont citées sur le dépôt source sans être trouvées dans le fichier:\n",
    "\n",
    "1. Ovide, Fastes\n",
    "2. Pétrone, Satyricon\n",
    "\n",
    "Nous remplacons les métamorphoses par les Fastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we align with filename from LASLA\n",
    "# Note that Petronius is mentionned there : https://github.com/PerseusDL/treebank_data/tree/7100a6b86826e121c6205182429ee670db64a392/v2.1/Latin\n",
    "# But not found (phi0972), neither Ovidius (0959)\n",
    "maps = {\n",
    "    # Phèdre, Fables\n",
    "    #      -> Non disponible\n",
    "    'phi0975.phi001.perseus-lat1.tb.xml': [],\n",
    "    # Auguste, Res Gestae\n",
    "    #      -> Non disponible\n",
    "    'phi1221.phi007.perseus-lat1.tb.xml': [],\n",
    "    # Suétone, Vie d'Auguste\n",
    "    #      -> Non disponible\n",
    "    #      -> Remplacé par Satyricon car mentionné\n",
    "    'phi1348.abo012.perseus-lat1.tb.xml': [\n",
    "    ],\n",
    "    # Tacite, Histoires\n",
    "    'phi1351.phi005.perseus-lat1.tb.xml': [\n",
    "        \"Tacitus_TacHistoriae_TacHist1.tsv\",\n",
    "        \"Tacitus_TacHistoriae_TacHist2.tsv\",\n",
    "        \"Tacitus_TacHistoriae_TacHist3.tsv\",\n",
    "        \"Tacitus_TacHistoriae_TacHist4.tsv\",\n",
    "        \"Tacitus_TacHistoriae_TacHist5.tsv\",\n",
    "    ],\n",
    "    # Vulgate\n",
    "    #      -> Non disponible\n",
    "    'tlg0031.tlg027.perseus-lat1.tb.xml': [\n",
    "    ],\n",
    "    # Caesar\n",
    "    'phi0448.phi001.perseus-lat1.tb.xml': [\n",
    "        \"Caesar_BellumGallicum_CaesBG1.tsv\",\n",
    "        \"Caesar_BellumGallicum_CaesBG2.tsv\",\n",
    "        \"Caesar_BellumGallicum_CaesBG3.tsv\",\n",
    "        \"Caesar_BellumGallicum_CaesBG4.tsv\",\n",
    "        \"Caesar_BellumGallicum_CaesBG5.tsv\",\n",
    "        \"Caesar_BellumGallicum_CaesBG6.tsv\",\n",
    "        \"Caesar_BellumGallicum_CaesBG7.tsv\"\n",
    "    ],\n",
    "    # Cicero, In Catilinam\n",
    "    'phi0474.phi013.perseus-lat1.tb.xml': [\n",
    "        \"Cicero_InCatilinam_CicCat1.tsv\",\n",
    "        \"Cicero_InCatilinam_CicCat2.tsv\",\n",
    "        \"Cicero_InCatilinam_CicCat3.tsv\",\n",
    "        \"Cicero_InCatilinam_CicCat4.tsv\"\n",
    "    ],\n",
    "    # Properce\n",
    "    'phi0620.phi001.perseus-lat1.tb.xml': [\n",
    "        \"Propertius_PropertiusElegiae_Propert1.tsv\",\n",
    "        \"Propertius_PropertiusElegiae_Propert2.tsv\",\n",
    "        \"Propertius_PropertiusElegiae_Propert3.tsv\",\n",
    "        \"Propertius_PropertiusElegiae_Propert4.tsv\"\n",
    "    ],\n",
    "    # Salluste, \n",
    "    'phi0631.phi001.perseus-lat1.tb.xml': [\n",
    "        \"Sallustius_Catilina_SalCatil.tsv\"\n",
    "    ],\n",
    "    # Virgile, \n",
    "    'phi0690.phi003.perseus-lat1.tb.xml': [\n",
    "        \"Vergilius_Aeneis_VerAen01.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen02.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen03.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen04.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen05.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen06.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen07.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen08.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen09.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen10.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen11.tsv\",\n",
    "        \"Vergilius_Aeneis_VerAen12.tsv\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'phi0975.phi001.perseus-lat1.tb.xml': \"Phèdre, Fables\",\n",
    "    'phi1221.phi007.perseus-lat1.tb.xml': \"Auguste, Res Gestae\",\n",
    "    'phi1348.abo012.perseus-lat1.tb.xml': \"Suétone, Vie d'Auguste\",\n",
    "    'phi1351.phi005.perseus-lat1.tb.xml': \"Tacite, Histoires\",\n",
    "    'tlg0031.tlg027.perseus-lat1.tb.xml': \"Vulgate\",\n",
    "    'phi0448.phi001.perseus-lat1.tb.xml': \"Caesar\",\n",
    "    'phi0474.phi013.perseus-lat1.tb.xml': \"Cicero, In Catilinam\",\n",
    "    'phi0620.phi001.perseus-lat1.tb.xml': \"Properce\",\n",
    "    'phi0631.phi001.perseus-lat1.tb.xml': \"Salluste, Catilina\",\n",
    "    'phi0690.phi003.perseus-lat1.tb.xml': \"Virgile, Énéide\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Title                 </td><td>Chunks</td><td>Tokens</td></tr>\n",
       "<tr><td>Auguste, Res Gestae   </td><td>38    </td><td>708   </td></tr>\n",
       "<tr><td>Caesar                </td><td>24    </td><td>352   </td></tr>\n",
       "<tr><td>Cicero, In Catilinam  </td><td>137   </td><td>1897  </td></tr>\n",
       "<tr><td>Phèdre, Fables        </td><td>233   </td><td>2397  </td></tr>\n",
       "<tr><td>Properce              </td><td>224   </td><td>2776  </td></tr>\n",
       "<tr><td>Salluste, Catilina    </td><td>336   </td><td>4999  </td></tr>\n",
       "<tr><td>Suétone, Vie d'Auguste</td><td>109   </td><td>2046  </td></tr>\n",
       "<tr><td>Tacite, Histoires     </td><td>64    </td><td>866   </td></tr>\n",
       "<tr><td>Virgile, Énéide       </td><td>15    </td><td>142   </td></tr>\n",
       "<tr><td>Vulgate               </td><td>154   </td><td>2001  </td></tr>\n",
       "<tr><td>Total                 </td><td>1334  </td><td>18184 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\hline\n",
      " Title                  & Chunks & Tokens \\\\\n",
      " Auguste, Res Gestae    & 38     & 708    \\\\\n",
      " Caesar                 & 24     & 352    \\\\\n",
      " Cicero, In Catilinam   & 137    & 1897   \\\\\n",
      " Phèdre, Fables         & 233    & 2397   \\\\\n",
      " Properce               & 224    & 2776   \\\\\n",
      " Salluste, Catilina     & 336    & 4999   \\\\\n",
      " Suétone, Vie d'Auguste & 109    & 2046   \\\\\n",
      " Tacite, Histoires      & 64     & 866    \\\\\n",
      " Virgile, Énéide        & 15     & 142    \\\\\n",
      " Vulgate                & 154    & 2001   \\\\\n",
      " Total                  & 1334   & 18184  \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Print table\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "rows = [[\"Title\", \"Chunks\", \"Tokens\"]]\n",
    "# Sort by author\n",
    "titles_rows = sorted(list(titles.items()), key=lambda x: x[1])\n",
    "\n",
    "totals = [0, 0]\n",
    "\n",
    "for key, title in titles_rows:\n",
    "    rows.append([title, chunks[key], tokens[key]])\n",
    "    totals[0] += chunks[key]\n",
    "    totals[1] += tokens[key]\n",
    "\n",
    "rows.append([\"Total\", *totals])\n",
    "display(HTML(tabulate.tabulate(rows, tablefmt='html')))\n",
    "\n",
    "print(tabulate.tabulate(rows, tablefmt='latex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\hline\n",
      " Petronius\\_PetroniusSatiricon\\_PetronSa.tsv & 136 \\\\\n",
      " Ovidius\\_Fasti\\_OvFasti1.tsv                & 136 \\\\\n",
      " Ovidius\\_Fasti\\_OvFasti2.tsv                & 136 \\\\\n",
      " Ovidius\\_Fasti\\_OvFasti3.tsv                & 136 \\\\\n",
      " Tacitus\\_TacHistoriae\\_TacHist1.tsv         &  13 \\\\\n",
      " Tacitus\\_TacHistoriae\\_TacHist2.tsv         &  13 \\\\\n",
      " Tacitus\\_TacHistoriae\\_TacHist3.tsv         &  13 \\\\\n",
      " Tacitus\\_TacHistoriae\\_TacHist4.tsv         &  13 \\\\\n",
      " Tacitus\\_TacHistoriae\\_TacHist5.tsv         &  13 \\\\\n",
      " Caesar\\_BellumGallicum\\_CaesBG1.tsv         &   4 \\\\\n",
      " Caesar\\_BellumGallicum\\_CaesBG2.tsv         &   4 \\\\\n",
      " Caesar\\_BellumGallicum\\_CaesBG3.tsv         &   4 \\\\\n",
      " Caesar\\_BellumGallicum\\_CaesBG4.tsv         &   4 \\\\\n",
      " Caesar\\_BellumGallicum\\_CaesBG5.tsv         &   4 \\\\\n",
      " Caesar\\_BellumGallicum\\_CaesBG6.tsv         &   4 \\\\\n",
      " Caesar\\_BellumGallicum\\_CaesBG7.tsv         &   4 \\\\\n",
      " Cicero\\_InCatilinam\\_CicCat1.tsv            &  35 \\\\\n",
      " Cicero\\_InCatilinam\\_CicCat2.tsv            &  35 \\\\\n",
      " Cicero\\_InCatilinam\\_CicCat3.tsv            &  35 \\\\\n",
      " Cicero\\_InCatilinam\\_CicCat4.tsv            &  35 \\\\\n",
      " Propertius\\_PropertiusElegiae\\_Propert1.tsv &  56 \\\\\n",
      " Propertius\\_PropertiusElegiae\\_Propert2.tsv &  56 \\\\\n",
      " Propertius\\_PropertiusElegiae\\_Propert3.tsv &  56 \\\\\n",
      " Propertius\\_PropertiusElegiae\\_Propert4.tsv &  56 \\\\\n",
      " Sallustius\\_Catilina\\_SalCatil.tsv          & 336 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen01.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen02.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen03.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen04.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen05.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen06.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen07.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen08.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen09.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen10.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen11.tsv             &   2 \\\\\n",
      " Vergilius\\_Aeneis\\_VerAen12.tsv             &   2 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Remplacement d'element\n",
    "replacements = [\n",
    "    \"Petronius_PetroniusSatiricon_PetronSa.tsv\",\n",
    "    \"Ovidius_Fasti_OvFasti1.tsv\",\n",
    "    \"Ovidius_Fasti_OvFasti2.tsv\",\n",
    "    \"Ovidius_Fasti_OvFasti3.tsv\"\n",
    "]\n",
    "use = Counter()\n",
    "import math\n",
    "\n",
    "for key, val in maps.items():\n",
    "    if not val:\n",
    "        val = replacements  # We use the replacements list if we have no mapping\n",
    "    nb_files = len(val)\n",
    "    nb_chunks = math.ceil(chunks[key]/nb_files)\n",
    "    for file in val:\n",
    "        use[file] += nb_chunks\n",
    "        \n",
    "\n",
    "print(tabulate.tabulate(list(use.items()), tablefmt='latex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_used = {\"tokens\": Counter(), \"chunks\": Counter()}\n",
    "test_used = {\"tokens\": Counter(), \"chunks\": Counter()}\n",
    "dev_used = {\"tokens\": Counter(), \"chunks\": Counter()}\n",
    "files = {}\n",
    "factor = 1\n",
    "with open(\"../../../LASLA/mood-tense-voice/perseus-train.tsv\", \"w\") as train:\n",
    "    with open(\"../../../LASLA/mood-tense-voice/perseus-test.tsv\", \"w\") as test:\n",
    "        with open(\"../../../LASLA/mood-tense-voice/perseus-dev.tsv\", \"w\") as dev:\n",
    "            header_written = False\n",
    "            for file, nb_chunks in use.items():\n",
    "                sentences = []\n",
    "                tokens = []\n",
    "                with open(\"../../../LASLA/mood-tense-voice/train/\"+file) as f:\n",
    "                    for line_no, line in enumerate(f):\n",
    "                        line = line.strip()\n",
    "                        if line_no == 0:\n",
    "                            if header_written == False:\n",
    "                                train.write(line+\"\\n\")\n",
    "                                test.write(line+\"\\n\")\n",
    "                                dev.write(line+\"\\n\")\n",
    "                            header = line.split()\n",
    "                            continue\n",
    "                        if not line:\n",
    "                            if tokens:\n",
    "                                sentences.append(tokens)\n",
    "                                tokens = []\n",
    "                            continue\n",
    "                        line_dict = dict(zip(header, line.split()))\n",
    "                        tokens.append(line_dict)\n",
    "                random.shuffle(sentences)\n",
    "                # Write train sentences\n",
    "                for sentence in sentences[:nb_chunks]:\n",
    "                    for tok in sentence:\n",
    "                        train.write(\"\\t\".join([tok[head] for head in header])+\"\\n\")\n",
    "                        train_used[\"tokens\"][file] += 1\n",
    "                    train.write(\"\\n\\n\")\n",
    "                    train_used[\"chunks\"][file] += 1\n",
    "\n",
    "                # Write dev sentence\n",
    "                nb_chunks_dev = math.ceil(0.1 * nb_chunks)\n",
    "                # We take at least one sentence, but 10% of the number of expected chunks\n",
    "                if nb_chunks_dev + nb_chunks <= len(sentences):\n",
    "                    # We can write dev !\n",
    "                    for sentence in sentences[nb_chunks:nb_chunks+nb_chunks_dev]:\n",
    "                        for tok in sentence:\n",
    "                            dev.write(\"\\t\".join([tok[head] for head in header])+\"\\n\")\n",
    "                            dev_used[\"tokens\"][file] += 1\n",
    "                        dev.write(\"\\n\\n\")\n",
    "                        dev_used[\"chunks\"][file] += 1\n",
    "                     \n",
    "                # Write the same amount in test if possible\n",
    "                nb_chunks_test = math.ceil(0.2 * nb_chunks)\n",
    "                nb_chunks_test = min(len(sentences), nb_chunks_dev+nb_chunks+nb_chunks_test)\n",
    "                if nb_chunks_dev+nb_chunks < len(sentences):\n",
    "                    for sentence in sentences[nb_chunks_dev+nb_chunks:nb_chunks_test]:\n",
    "                        for tok in sentence:\n",
    "                            test.write(\"\\t\".join([tok[head] for head in header])+\"\\n\")\n",
    "                            test_used[\"tokens\"][file] += 1\n",
    "                        test.write(\"\\n\\n\")\n",
    "                        test_used[\"chunks\"][file] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllll}\n",
      "\\hline\n",
      " File         & Tokens &      &      & Chunks &     &      \\\\\n",
      "              & Train  & Dev  & Test & Train  & Dev & Test \\\\\n",
      " Petron, Sa   & 1955   & 176  & 402  & 136    & 14  & 28   \\\\\n",
      " Ov, Fasti, 1 & 2251   & 208  & 455  & 136    & 14  & 28   \\\\\n",
      " Ov, Fasti, 2 & 1901   & 186  & 445  & 136    & 14  & 28   \\\\\n",
      " Ov, Fasti, 3 & 2225   & 154  & 439  & 136    & 14  & 28   \\\\\n",
      " Tac, Hist, 1 & 281    & 72   & 51   & 13     & 2   & 3    \\\\\n",
      " Tac, Hist, 2 & 251    & 70   & 88   & 13     & 2   & 3    \\\\\n",
      " Tac, Hist, 3 & 269    & 40   & 57   & 13     & 2   & 3    \\\\\n",
      " Tac, Hist, 4 & 592    & 30   & 95   & 13     & 2   & 3    \\\\\n",
      " Tac, Hist, 5 & 516    & 126  & 144  & 13     & 2   & 3    \\\\\n",
      " Caes, BG, 1  & 146    & 26   & 54   & 4      & 1   & 1    \\\\\n",
      " Caes, BG, 2  & 110    & 63   & 23   & 4      & 1   & 1    \\\\\n",
      " Caes, BG, 3  & 76     & 32   & 19   & 4      & 1   & 1    \\\\\n",
      " Caes, BG, 4  & 99     & 32   & 30   & 4      & 1   & 1    \\\\\n",
      " Caes, BG, 5  & 73     & 10   & 16   & 4      & 1   & 1    \\\\\n",
      " Caes, BG, 6  & 132    & 7    & 8    & 4      & 1   & 1    \\\\\n",
      " Caes, BG, 7  & 69     & 15   & 22   & 4      & 1   & 1    \\\\\n",
      " Cic, Cat, 1  & 705    & 90   & 157  & 35     & 4   & 7    \\\\\n",
      " Cic, Cat, 2  & 819    & 101  & 138  & 35     & 4   & 7    \\\\\n",
      " Cic, Cat, 3  & 1014   & 149  & 102  & 35     & 4   & 7    \\\\\n",
      " Cic, Cat, 4  & 933    & 111  & 192  & 35     & 4   & 7    \\\\\n",
      " Propert, 1   & 952    & 113  & 250  & 56     & 6   & 12   \\\\\n",
      " Propert, 2   & 1002   & 75   & 314  & 56     & 6   & 12   \\\\\n",
      " Propert, 3   & 875    & 67   & 258  & 56     & 6   & 12   \\\\\n",
      " Propert, 4   & 1029   & 88   & 149  & 56     & 6   & 12   \\\\\n",
      " Sal, Catil   & 7389   & 617  & 1562 & 336    & 34  & 67   \\\\\n",
      " Ver, Aen, 1  & 27     & 29   & 7    & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 2  & 24     & 29   & 27   & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 3  & 42     & 39   & 24   & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 4  & 35     & 9    & 56   & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 5  & 41     & 33   & 20   & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 6  & 30     & 49   & 22   & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 7  & 31     & 25   & 6    & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 8  & 43     & 19   & 30   & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 9  & 19     & 35   & 15   & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 10 & 47     & 44   & 3    & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 11 & 33     & 14   & 31   & 2      & 1   & 1    \\\\\n",
      " Ver, Aen, 12 & 45     & 15   & 7    & 2      & 1   & 1    \\\\\n",
      " Total        & 26081  & 2998 & 5718 & 1361   & 159 & 289  \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Get the table\n",
    "import re\n",
    "def get_title(filename):\n",
    "    for group in re.findall(r\"([A-Z][a-z]+)([A-Z][a-zA-Z]+)?0?(\\d+)?\\.tsv\", filename):\n",
    "        return \", \".join([g for g in group if g])\n",
    "\n",
    "rows = [[\"File\", \"Tokens\", \"\", \"\", \"Chunks\", \"\", \"\"], [\"\", \"Train\", \"Dev\", \"Test\", \"Train\", \"Dev\", \"Test\"]]\n",
    "total = {\"chunks\": {\"train\": 0, \"dev\": 0, \"test\": 0}, \"tokens\": {\"train\": 0, \"dev\": 0, \"test\": 0}}\n",
    "for file in use:\n",
    "    rows.append([get_title(file)])\n",
    "    for cat in [\"tokens\", \"chunks\"]:\n",
    "        for corpus, counter in {\"train\": train_used, \"dev\": dev_used, \"test\": test_used}.items():\n",
    "            rows[-1].append(counter[cat][file])\n",
    "            total[cat][corpus] += counter[cat][file]\n",
    "\n",
    "rows.append(\n",
    "    [\"Total\"] + \\\n",
    "    [total[cat][corpus] for cat in [\"tokens\", \"chunks\"] for corpus in total[cat]]\n",
    "\n",
    ")\n",
    "            \n",
    "            \n",
    "print(tabulate.tabulate(rows, tablefmt='latex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the configuration\n",
    "# Create the configuration files for all files\n",
    "import json\n",
    "\n",
    "BASE_CONFIG = json.loads(\"\"\"{\n",
    "    \"modelname\": \"model-perseus-\",\n",
    "    \"modelpath\": \"./models/\",\n",
    "    \"run_test\": false,\n",
    "    \"max_sent_len\": 35,\n",
    "    \"max_sents\": 1000000,\n",
    "    \"input_path\": \"./protogenie-partial/train-perseus.tsv\",\n",
    "    \"test_path\": \"./protogenie-partial/test-perseus.tsv\",\n",
    "    \"dev_path\": \"./protogenie-partial/dev-perseus.tsv\",\n",
    "    \"breakline_ref\": \"pos\",\n",
    "    \"breakline_data\": \"$.\",\n",
    "    \"char_max_size\": 500,\n",
    "    \"word_max_size\": 20000,\n",
    "    \"char_min_freq\": 1,\n",
    "    \"word_min_freq\": 1,\n",
    "    \"char_eos\": true,\n",
    "    \"char_bos\": true,\n",
    "    \"header\": true,\n",
    "    \"sep\": \"\\\\t\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"name\": \"lemma\",\n",
    "            \"target\": true,\n",
    "            \"context\": \"sentence\",\n",
    "            \"level\": \"char\",\n",
    "            \"decoder\": \"attentional\",\n",
    "            \"settings\": {\n",
    "                \"bos\": true,\n",
    "                \"eos\": true,\n",
    "                \"lower\": true,\n",
    "                \"target\": \"lemma\"\n",
    "            },\n",
    "            \"layer\": -1\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"pos\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Gend\"\n",
    "        }\n",
    "    ],\n",
    "    \"task_defaults\": {\n",
    "        \"level\": \"token\",\n",
    "        \"layer\": -1,\n",
    "        \"decoder\": \"linear\",\n",
    "        \"context\": \"sentence\"\n",
    "    },\n",
    "    \"patience\": 5,\n",
    "    \"factor\": 0.5,\n",
    "    \"threshold\": 0.0001,\n",
    "    \"min_weight\": 0.2,\n",
    "    \"include_lm\": true,\n",
    "    \"lm_shared_softmax\": true,\n",
    "    \"lm_schedule\": {\n",
    "        \"patience\": 2,\n",
    "        \"factor\": 0.5,\n",
    "        \"weight\": 0.2,\n",
    "        \"mode\": \"min\"\n",
    "    },\n",
    "    \"batch_size\": 256,\n",
    "    \"dropout\": 0.25,\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_factor\": 0.5,\n",
    "    \"lr_patience\": 2,\n",
    "    \"epochs\": 100,\n",
    "    \"cell\": \"GRU\",\n",
    "    \"num_layers\": 1,\n",
    "    \"hidden_size\": 128,\n",
    "    \"wemb_dim\": 100,\n",
    "    \"cemb_dim\": 150,\n",
    "    \"cemb_type\": \"rnn\",\n",
    "    \"cemb_layers\": 2,\n",
    "    \"checks_per_epoch\": 1,\n",
    "    \"report_freq\": 200,\n",
    "    \"verbose\": true,\n",
    "    \"device\": \"cuda\",\n",
    "    \"buffer_size\": 10000,\n",
    "    \"minimize_pad\": false,\n",
    "    \"word_dropout\": 0,\n",
    "    \"shuffle\": true,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"clip_norm\": 5,\n",
    "    \"pretrain_embeddings\": false,\n",
    "    \"load_pretrained_embeddings\": \"\",\n",
    "    \"load_pretrained_encoder\": \"\",\n",
    "    \"freeze_embeddings\": false,\n",
    "    \"custom_cemb_cell\": false,\n",
    "    \"merge_type\": \"concat\",\n",
    "    \"scorer\": \"general\",\n",
    "    \"linear_layers\": 1\n",
    "}\"\"\")\n",
    "# Reduced Cemb DIM from 300 to 150\n",
    "with open(\"../../../LASLA/configs/perseus-train.json\", \"w\") as f:\n",
    "    json.dump(BASE_CONFIG, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
