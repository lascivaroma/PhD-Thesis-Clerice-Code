{"modelname": "model-1.4.5.d-seneque-philo-autres--", "modelpath": "./models/", "run_test": false, "max_sent_len": 35, "max_sents": 1000000, "input_path": "./1.4.5.d-lemmatisation-impact/seneque-philo-autres-train.tsv", "dev_path": "./1.4.5.d-lemmatisation-impact/seneque-philo-autres-dev.tsv", "breakline_ref": "pos", "breakline_data": "$.", "char_max_size": 500, "word_max_size": 20000, "char_min_freq": 1, "word_min_freq": 1, "char_eos": true, "char_bos": true, "header": true, "sep": "\t", "tasks": [{"name": "lemma", "target": true, "context": "sentence", "level": "char", "decoder": "attentional", "settings": {"bos": true, "eos": true, "lower": true, "target": "lemma"}, "layer": -1}, {"name": "pos"}, {"name": "Gend"}], "task_defaults": {"level": "token", "layer": -1, "decoder": "linear", "context": "sentence"}, "batch_size": 64, "dropout": 0.25, "lr": 0.001, "lr_factor": 0.5, "lr_patience": 10, "patience": 8, "factor": 0.5, "threshold": 0.0001, "min_weight": 0.2, "include_lm": true, "lm_shared_softmax": true, "lm_schedule": {"patience": 2, "factor": 0.5, "weight": 0.2, "mode": "min"}, "epochs": 100, "cell": "GRU", "num_layers": 1, "hidden_size": 128, "wemb_dim": 100, "cemb_dim": 300, "cemb_type": "rnn", "cemb_layers": 2, "checks_per_epoch": 1, "report_freq": 200, "verbose": true, "device": "cuda", "buffer_size": 10000, "minimize_pad": false, "word_dropout": 0, "shuffle": true, "optimizer": "Adam", "clip_norm": 5, "pretrain_embeddings": false, "load_pretrained_embeddings": "", "load_pretrained_encoder": "", "freeze_embeddings": false, "custom_cemb_cell": false, "merge_type": "concat", "scorer": "general", "linear_layers": 1}