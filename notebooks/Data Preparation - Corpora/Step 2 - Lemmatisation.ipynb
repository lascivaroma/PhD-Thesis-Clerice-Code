{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAIN_TEXT_FILES_PATH = \"lemmatized/plain-text/*.txt\"\n",
    "CHECKSUM_PATH = \"./checksum-xmls.csv\"\n",
    "FORCE_RELEMMATIZE = False # Force relemmatization\n",
    "DOWNLOAD_MODELS = False # Update the model files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage et upgrade\n",
    "\n",
    "Potentiellement besoin de :\n",
    "\n",
    "```shell\n",
    "!pip install https://github.com/PonteIneptique/pie/archive/improvement/AttentionDecoder.predict_max.zip#egg=nlp_pie --upgrade\n",
    "!pip install --upgrade https://github.com/hipster-philology/nlp-pie-taggers/archive/feature/hashlist.zip#egg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './lemmatized/plain-text/*-pie*.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm ./lemmatized/plain-text/*-pie*.txt\n",
    "#!pip install --upgrade pie-extended\n",
    "#!pip install --upgrade https://github.com/PonteIneptique/pie/archive/improvement/AttentionDecoder.predict_max.zip#egg=nlp_pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pie-extended download lasla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des textes à lemmatiser\n",
    "\n",
    "## Lecture du fichier des checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841 former source file\n",
      "841 target files\n",
      "16 changed input files detected\n",
      "0 new input files detected\n"
     ]
    }
   ],
   "source": [
    "from hash_compute import md5sum, check_checksum_from_file, read_checksum_csv\n",
    "import os.path\n",
    "import glob\n",
    "\n",
    "target_files = glob.glob(PLAIN_TEXT_FILES_PATH)\n",
    "existing_files = {\n",
    "    os.path.abspath(target): md5sum(target)\n",
    "    for target in target_files\n",
    "}\n",
    "\n",
    "former_checksums = read_checksum_csv(CHECKSUM_PATH)\n",
    "plaintext_former = [\n",
    "    os.path.abspath(element)\n",
    "    for element in former_checksums.keys()\n",
    "]\n",
    "#print(former_checksums)\n",
    "new_input = [\n",
    "    target\n",
    "    for target in existing_files\n",
    "    if target not in plaintext_former\n",
    "]\n",
    "changed_input, _ = check_checksum_from_file(CHECKSUM_PATH)\n",
    "\n",
    "\n",
    "print(f\"{len(former_checksums)} former source file\")\n",
    "print(f\"{len(target_files)} target files\")\n",
    "print(f\"{len(changed_input)} changed input files detected\")\n",
    "print(f\"{len(new_input)} new input files detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de la liste des fichiers à produire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 files to lemmatize\n"
     ]
    }
   ],
   "source": [
    "texts_to_lemmatize = []\n",
    "\n",
    "if FORCE_RELEMMATIZE:\n",
    "    texts_to_lemmatize = list(existing_files.keys())\n",
    "else:\n",
    "    texts_to_lemmatize = new_input + changed_input\n",
    "    \n",
    "print(f\"{len(texts_to_lemmatize)} files to lemmatize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation\n",
    "\n",
    "## Initialisation du tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASLA Version 0.0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from pie_extended.cli.utils import get_tagger, get_model, download\n",
    "\n",
    "if DOWNLOAD_MODELS:\n",
    "    for dl in download(\"lasla\"):\n",
    "        x = 1\n",
    "\n",
    "from pie_extended.models.lasla import VERSION\n",
    "print(f\"LASLA Version {VERSION}\")\n",
    "# model_path allows you to override the model loaded by another .tar\n",
    "model_name = \"lasla\"\n",
    "tagger = get_tagger(model_name, batch_size=64, device=\"cuda\", model_path=None)\n",
    "tagger.lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taggage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [08:39<00:00, 32.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the main object from the model (: data iterator + postprocesor\n",
    "from pie_extended.models.lasla.imports import get_iterator_and_processor\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "for file in tqdm.tqdm(texts_to_lemmatize):\n",
    "    try:\n",
    "        if \"-pie\" not in file:\n",
    "            iterator, processor = get_iterator_and_processor(max_tokens=64)\n",
    "            tagger.tag_file(file, iterator=iterator, processor=processor)\n",
    "    except Exception as E:\n",
    "        print(file)\n",
    "        print(E)\n",
    "        raise E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the main object from the model (: data iterator + postprocesor\n",
    "from pie_extended.models.lasla.imports import get_iterator_and_processor\n",
    "\n",
    "iterator, processor = get_iterator_and_processor()\n",
    "file = \"lemmatized/plain-text/urn:cts:latinLit:stoa0275.stoa006.opp-lat1.txt\"\n",
    "#with open(file) as f:\n",
    "#    print(f.read())\n",
    "\n",
    "#tagger.tag_file(file, iterator=iterator, processor=processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déplacement des fichiers dans le dossier TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p lemmatized/tsv\n",
    "!mv lemmatized/plain-text/*-pie.txt lemmatized/tsv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update de l'index des fichiers à lemmatiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified files\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0186.stoa001.digilibLT-lat1.txt', is_source=False, checksum='838dc3017e9e844d93ff02ceaa9610c1')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0186.stoa002.digilibLT-lat1.txt', is_source=False, checksum='d1b650722fa2dd7b84d329864df78263')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0123a.stoa002.digilibLT-lat1.txt', is_source=False, checksum='c99e42ddfa1df41eb3fbd3693edeebd7')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0128e.stoa001.digilibLT-lat1.txt', is_source=False, checksum='509bed23e5125118c8fe5d7d0b569cc5')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi996.phi004.digilibLT-lat1.txt', is_source=False, checksum='627474c69dede0e066d5b5e0685b5c92')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0012a.stoa003.digilibLT-lat1.txt', is_source=False, checksum='471f33149a88137cd35411809764550a')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0012a.stoa001.digilibLT-lat1.txt', is_source=False, checksum='c55568c8911275606d7d031c6adb9b67')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0128b.stoa004.digilibLT-lat1.txt', is_source=False, checksum='f2aa343e30a66749d0c65393988b2fda')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0228b.stoa002.digilibLT-lat1.txt', is_source=False, checksum='998f5ac0f07501c0808dcad5e9d976e2')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0187a.stoa001.digilibLT-lat1.txt', is_source=False, checksum='1cece5f8704ad9e6b14f585136409284')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0020b.stoa001.digilibLT-lat1.txt', is_source=False, checksum='1310e80062663b227d21f00a8fdfea47')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0043.stoa001.digilibLT-lat1.txt', is_source=False, checksum='ac914e42aecf1014a6a9a0334c879c7c')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0023.stoa001.digilibLT-lat1.txt', is_source=False, checksum='256534aed433abde5ff106542d281771')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0171a.stoa001.digilibLT-lat1.txt', is_source=False, checksum='b46c3dd274871d8072883222c532992f')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0329.stoa001.digilibLT-lat1.txt', is_source=False, checksum='5cebbb29449938408d0df74da656a0ad')\n",
      "ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:stoa0357c.stoa004.digilibLT-lat1.txt', is_source=False, checksum='62e62fbc7ecd4331de5affaa872d2f71')\n"
     ]
    }
   ],
   "source": [
    "## ToDo When there is a new update of the corpus\n",
    "from hash_compute import write_csv_checksums\n",
    "\n",
    "rows, modified = write_csv_checksums(CHECKSUM_PATH, texts_to_lemmatize, _write=True)\n",
    "\n",
    "print(\"Modified files\")\n",
    "for mod in modified:\n",
    "    print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des fichiers à re-xmliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FORCE_RELEMMATIZE:\n",
    "    with open(\"new_xml.txt\", \"w\") as f:\n",
    "        for mod in texts_to_lemmatize:\n",
    "            f.write(\n",
    "                f\"{os.path.relpath(mod.replace('.txt', '-pie.txt').replace('plain-text', 'tsv'))}\\n\"\n",
    "            )\n",
    "else:\n",
    "    with open(\"new_xml.txt\", \"w\") as f:\n",
    "        for mod in modified:\n",
    "            f.write(f\"{os.path.relpath(mod.filename.replace('.txt', '-pie.txt').replace('plain-text', 'tsv'))}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "375.467px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
