{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAIN_TEXT_FILES_PATH = \"lemmatized/plain-text/*.txt\"\n",
    "CHECKSUM_PATH = \"./checksum-xmls.csv\"\n",
    "FORCE_RELEMMATIZE = False # Force relemmatization\n",
    "DOWNLOAD_MODELS = False # Update the model files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage et upgrade\n",
    "\n",
    "Potentiellement besoin de :\n",
    "\n",
    "```shell\n",
    "!pip install https://github.com/PonteIneptique/pie/archive/improvement/AttentionDecoder.predict_max.zip#egg=nlp_pie --upgrade\n",
    "!pip install --upgrade https://github.com/hipster-philology/nlp-pie-taggers/archive/feature/hashlist.zip#egg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './lemmatized/plain-text/*-pie*.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm ./lemmatized/plain-text/*-pie*.txt\n",
    "#!pip install --upgrade https://github.com/hipster-philology/nlp-pie-taggers/archive/feature/hashlist.zip#egg\n",
    "#!pip install --upgrade https://github.com/PonteIneptique/pie/archive/improvement/AttentionDecoder.predict_max.zip#egg=nlp_pie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des textes à lemmatiser\n",
    "\n",
    "## Lecture du fichier des checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652 former source file\n",
      "652 target files\n",
      "0 changed input files detected\n",
      "0 new input files detected\n"
     ]
    }
   ],
   "source": [
    "from pie_extended.utils.hashcheck import md5sum, check_checksum_from_file, read_checksum_csv\n",
    "import os.path\n",
    "\n",
    "target_files = glob.glob(PLAIN_TEXT_FILES_PATH)\n",
    "existing_files = {\n",
    "    os.path.abspath(target): md5sum(target)\n",
    "    for target in target_files\n",
    "}\n",
    "\n",
    "former_checksums = read_checksum_csv(CHECKSUM_PATH)\n",
    "new_input = [target for target in existing_files if target not in former_checksums]\n",
    "changed_input, _ = check_checksum_from_file(CHECKSUM_PATH)\n",
    "\n",
    "\n",
    "print(f\"{len(former_checksums)} former source file\")\n",
    "print(f\"{len(target_files)} target files\")\n",
    "print(f\"{len(changed_input)} changed input files detected\")\n",
    "print(f\"{len(new_input)} new input files detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de la liste des fichiers à produire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files to lemmatize\n"
     ]
    }
   ],
   "source": [
    "texts_to_lemmatize = []\n",
    "\n",
    "if FORCE_RELEMMATIZE:\n",
    "    texts_to_lemmatize = list(existing_files.keys())\n",
    "else:\n",
    "    texts_to_lemmatize = new_input + changed_input\n",
    "    \n",
    "print(f\"{len(texts_to_lemmatize)} files to lemmatize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation\n",
    "\n",
    "## Initialisation du tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pie_extended.cli.utils import get_tagger, get_model, download\n",
    "\n",
    "if DOWNLOAD_MODELS:\n",
    "    for dl in download(\"lasla\"):\n",
    "        x = 1\n",
    "\n",
    "# model_path allows you to override the model loaded by another .tar\n",
    "model_name = \"lasla\"\n",
    "tagger = get_tagger(model_name, batch_size=128, device=\"cuda\", model_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taggage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "# Get the main object from the model (: data iterator + postprocesor\n",
    "from pie_extended.models.lasla.imports import get_iterator_and_processor\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "for file in tqdm.tqdm(texts_to_lemmatize):\n",
    "    try:\n",
    "        if \"-pie\" not in file:\n",
    "            \n",
    "            iterator, processor = get_iterator_and_processor()\n",
    "            tagger.tag_file(file, iterator=iterator, processor=processor)\n",
    "    except Exception as E:\n",
    "        print(file)\n",
    "        print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the main object from the model (: data iterator + postprocesor\n",
    "from pie_extended.models.lasla.imports import get_iterator_and_processor\n",
    "\n",
    "iterator, processor = get_iterator_and_processor()\n",
    "file = \"lemmatized/plain-text/urn:cts:latinLit:stoa0275.stoa006.opp-lat1.txt\"\n",
    "#with open(file) as f:\n",
    "#    print(f.read())\n",
    "\n",
    "#tagger.tag_file(file, iterator=iterator, processor=processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déplacement des fichiers dans le dossier TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'lemmatized/plain-text/*-pie.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p lemmatized/tsv\n",
    "!mv lemmatized/plain-text/*-pie.txt lemmatized/tsv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update de l'index des fichiers à lemmatiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ToDo When there is a new update of the corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
