{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAIN_TEXT_FILES_PATH = \"lemmatized/plain-text/*.txt\"\n",
    "CHECKSUM_PATH = \"./checksum-xmls.csv\"\n",
    "FORCE_RELEMMATIZE = False # Force relemmatization\n",
    "DOWNLOAD_MODELS = False # Update the model files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage et upgrade\n",
    "\n",
    "Potentiellement besoin de :\n",
    "\n",
    "```shell\n",
    "!pip install https://github.com/PonteIneptique/pie/archive/improvement/AttentionDecoder.predict_max.zip#egg=nlp_pie --upgrade\n",
    "!pip install --upgrade https://github.com/hipster-philology/nlp-pie-taggers/archive/feature/hashlist.zip#egg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './lemmatized/plain-text/*-pie*.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm ./lemmatized/plain-text/*-pie*.txt\n",
    "#!pip install --upgrade pie-extended\n",
    "#!pip install --upgrade https://github.com/PonteIneptique/pie/archive/improvement/AttentionDecoder.predict_max.zip#egg=nlp_pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade --no-cache https://github.com/PonteIneptique/pie/archive/torch/upgrade-fix.zip#egg=nlp_pie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des textes à lemmatiser\n",
    "\n",
    "## Lecture du fichier des checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746 former source file\n",
      "746 target files\n",
      "3 changed input files detected\n",
      "0 new input files detected\n"
     ]
    }
   ],
   "source": [
    "from hash_compute import md5sum, check_checksum_from_file, read_checksum_csv\n",
    "import os.path\n",
    "import glob\n",
    "\n",
    "target_files = glob.glob(PLAIN_TEXT_FILES_PATH)\n",
    "existing_files = {\n",
    "    os.path.abspath(target): md5sum(target)\n",
    "    for target in target_files\n",
    "}\n",
    "\n",
    "former_checksums = read_checksum_csv(CHECKSUM_PATH)\n",
    "plaintext_former = [\n",
    "    os.path.abspath(element)\n",
    "    for element in former_checksums.keys()\n",
    "]\n",
    "#print(former_checksums)\n",
    "new_input = [\n",
    "    target\n",
    "    for target in existing_files\n",
    "    if target not in plaintext_former\n",
    "]\n",
    "changed_input, _ = check_checksum_from_file(CHECKSUM_PATH)\n",
    "\n",
    "\n",
    "print(f\"{len(former_checksums)} former source file\")\n",
    "print(f\"{len(target_files)} target files\")\n",
    "print(f\"{len(changed_input)} changed input files detected\")\n",
    "print(f\"{len(new_input)} new input files detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de la liste des fichiers à produire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 files to lemmatize\n"
     ]
    }
   ],
   "source": [
    "texts_to_lemmatize = []\n",
    "\n",
    "if FORCE_RELEMMATIZE:\n",
    "    texts_to_lemmatize = list(existing_files.keys())\n",
    "else:\n",
    "    texts_to_lemmatize = new_input + changed_input\n",
    "    \n",
    "print(f\"{len(texts_to_lemmatize)} files to lemmatize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation\n",
    "\n",
    "## Initialisation du tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASLA Version 0.0.5b\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pie_extended.cli.utils import get_tagger, get_model, download\n",
    "\n",
    "if DOWNLOAD_MODELS:\n",
    "    for dl in download(\"lasla\"):\n",
    "        x = 1\n",
    "\n",
    "from pie_extended.models.lasla import VERSION\n",
    "print(f\"LASLA Version {VERSION}\")\n",
    "# model_path allows you to override the model loaded by another .tar\n",
    "model_name = \"lasla\"\n",
    "tagger = get_tagger(model_name, batch_size=64, device=\"cuda\", model_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taggage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:00<00:00, 60.25s/it] \n"
     ]
    }
   ],
   "source": [
    "# Get the main object from the model (: data iterator + postprocesor\n",
    "from pie_extended.models.lasla.imports import get_iterator_and_processor\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "for file in tqdm.tqdm(texts_to_lemmatize):\n",
    "    try:\n",
    "        if \"-pie\" not in file:\n",
    "            iterator, processor = get_iterator_and_processor(max_tokens=64)\n",
    "            tagger.tag_file(file, iterator=iterator, processor=processor)\n",
    "    except Exception as E:\n",
    "        print(file)\n",
    "        print(E)\n",
    "        raise E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the main object from the model (: data iterator + postprocesor\n",
    "from pie_extended.models.lasla.imports import get_iterator_and_processor\n",
    "\n",
    "iterator, processor = get_iterator_and_processor()\n",
    "file = \"lemmatized/plain-text/urn:cts:latinLit:stoa0275.stoa006.opp-lat1.txt\"\n",
    "#with open(file) as f:\n",
    "#    print(f.read())\n",
    "\n",
    "#tagger.tag_file(file, iterator=iterator, processor=processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déplacement des fichiers dans le dossier TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p lemmatized/tsv\n",
    "!mv lemmatized/plain-text/*-pie.txt lemmatized/tsv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update de l'index des fichiers à lemmatiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified files\n",
      "[ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi005.perseus-lat1.txt', is_source=False, checksum='e9351896b85ff1d91c07c1f1fdf76681'), ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi007.perseus-lat1.txt', is_source=False, checksum='7916efcf4414fbd6dd4f0d9d3612d44d'), ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi006.perseus-lat1.txt', is_source=False, checksum='214c6a429c5666d532132a8be46d4629')]\n",
      "[ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi005.perseus-lat1.txt', is_source=False, checksum='e9351896b85ff1d91c07c1f1fdf76681'), ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi007.perseus-lat1.txt', is_source=False, checksum='7916efcf4414fbd6dd4f0d9d3612d44d'), ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi006.perseus-lat1.txt', is_source=False, checksum='214c6a429c5666d532132a8be46d4629')]\n",
      "[ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi005.perseus-lat1.txt', is_source=False, checksum='e9351896b85ff1d91c07c1f1fdf76681'), ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi007.perseus-lat1.txt', is_source=False, checksum='7916efcf4414fbd6dd4f0d9d3612d44d'), ModifiedFiles(filename='/home/thibault/dev/these/notebooks/Data Preparation - Corpora/lemmatized/plain-text/urn:cts:latinLit:phi2349.phi006.perseus-lat1.txt', is_source=False, checksum='214c6a429c5666d532132a8be46d4629')]\n"
     ]
    }
   ],
   "source": [
    "## ToDo When there is a new update of the corpus\n",
    "from hash_compute import write_csv_checksums\n",
    "\n",
    "rows, modified = write_csv_checksums(CHECKSUM_PATH, texts_to_lemmatize, _write=True)\n",
    "\n",
    "print(\"Modified files\")\n",
    "for mod in modified:\n",
    "    print(modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des fichiers à re-xmliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"new_xml.txt\", \"w\") as f:\n",
    "    for mod in modified:\n",
    "        f.write(f\"{os.path.relpath(mod.filename.replace('.txt', '-pie.txt').replace('plain-text', 'tsv'))}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
